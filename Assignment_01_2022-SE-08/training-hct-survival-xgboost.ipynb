{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":70942,"databundleVersionId":10381525,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-24T06:59:38.269682Z","iopub.execute_input":"2025-02-24T06:59:38.269968Z","iopub.status.idle":"2025-02-24T06:59:38.277280Z","shell.execute_reply.started":"2025-02-24T06:59:38.269942Z","shell.execute_reply":"2025-02-24T06:59:38.275972Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/equity-post-HCT-survival-predictions/sample_submission.csv\n/kaggle/input/equity-post-HCT-survival-predictions/data_dictionary.csv\n/kaggle/input/equity-post-HCT-survival-predictions/train.csv\n/kaggle/input/equity-post-HCT-survival-predictions/test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Training Notebook for XGBoost\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.impute import SimpleImputer\nimport xgboost as xgb\nimport joblib\n\n# Define selected columns\nselected_columns = [\n    \"prim_disease_hct\", \"hla_match_b_low\", \"prod_type\", \"year_hct\", \"obesity\", \n    \"donor_age\", \"prior_tumor\", \"gvhd_proph\", \"sex_match\", \"comorbidity_score\", \n    \"karnofsky_score\", \"donor_related\", \"age_at_hct\", \"efs\"  # Target column\n]\n\n# Load dataset\ntrain_file_path = \"/kaggle/input/equity-post-HCT-survival-predictions/train.csv\"\ndf = pd.read_csv(train_file_path)\n\n# Keep only selected columns\ndf = df[selected_columns]\n\n# Separate features and target\nX = df.drop(columns=[\"efs\"])\ny = df[\"efs\"]\n\n# Identify numerical and categorical columns\nnum_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\ncat_cols = X.select_dtypes(include=['object']).columns.tolist()\n\n# Handle missing values\nnum_imputer = SimpleImputer(strategy='median')\nX[num_cols] = num_imputer.fit_transform(X[num_cols])\n\ncat_imputer = SimpleImputer(strategy='most_frequent')\nX[cat_cols] = cat_imputer.fit_transform(X[cat_cols])\n\n# Encode categorical features\nencoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\nencoded_cats = encoder.fit_transform(X[cat_cols])\ncat_feature_names = encoder.get_feature_names_out(cat_cols)\nX_encoded = pd.DataFrame(encoded_cats, columns=cat_feature_names)\n\n# Drop original categorical columns and merge encoded ones\nX = X.drop(columns=cat_cols)\nX = pd.concat([X, X_encoded], axis=1)\n\n# Standardize numerical features\nscaler = StandardScaler()\nX[num_cols] = scaler.fit_transform(X[num_cols])\n\n# Save preprocessors for testing\njoblib.dump(num_imputer, \"num_imputer.pkl\")\njoblib.dump(cat_imputer, \"cat_imputer.pkl\")\njoblib.dump(encoder, \"encoder.pkl\")\njoblib.dump(scaler, \"scaler.pkl\")\n\n# Save numerical and categorical columns for testing\nimport joblib\njoblib.dump(num_cols, \"num_cols.pkl\")\njoblib.dump(cat_cols, \"cat_cols.pkl\")\nprint(\"Numerical and categorical columns saved!\")\n\n# Split into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Convert to DMatrix for XGBoost\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndval = xgb.DMatrix(X_val, label=y_val)\n\n# Save the list of columns used in the training dataset\nimport joblib\njoblib.dump(X.columns.tolist(), \"training_columns.pkl\")\nprint(\"Training columns saved!\")\n\n# Set XGBoost parameters\nparams = {\n    'objective': 'binary:logistic',\n    'eval_metric': 'logloss',\n    'max_depth': 6,\n    'eta': 0.1,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'seed': 42\n}\n\n# Train the model\nnum_round = 100\nbst = xgb.train(params, dtrain, num_round, evals=[(dval, 'validation')])\n\n# Save the model\nbst.save_model(\"xgboost_model.model\")\nprint(\"XGBoost model saved!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T12:19:15.044949Z","iopub.execute_input":"2025-02-23T12:19:15.045432Z","iopub.status.idle":"2025-02-23T12:19:16.516853Z","shell.execute_reply.started":"2025-02-23T12:19:15.045403Z","shell.execute_reply":"2025-02-23T12:19:16.515876Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}